\documentclass{beamer}

\usetheme{Madrid}
\usecolortheme{default}
\usepackage{amsmath, amssymb}

\title{Transformers for our problems}
\author{[Nareli] --- AI Applications Specialist}
\date{\today}

\begin{document}

%------------------------------------------------
\frame{\titlepage}
%------------------------------------------------

\begin{frame}{Core Idea of Transformers}
\begin{itemize}
    \item \textbf{Architecture:} Based on self-attention, allowing models to weigh relationships across all input tokens in parallel.
    \item \textbf{Advantage over RNNs/CNNs:} Handles long-range dependencies and scales better.
    \item \textbf{Applications:}
    \begin{itemize}
        \item Text: summarization, compliance, search
        \item Code: bug detection, code generation
        \item Time series: forecasting, anomaly detection
        \item Multimodal: integrating text, images, logs
    \end{itemize}
\end{itemize}
\end{frame}

%------------------------------------------------

\begin{frame}{Advantages for Enterprise Efficiency}
\begin{itemize}
    \item \textbf{Automation of Knowledge Work:} Report drafting, ticket classification.
    \item \textbf{Decision Support:} Context-aware recommendations, anomaly detection.
    \item \textbf{Scalability:} Pretrained models reduce data requirements.
    \item \textbf{Integration:} Can be paired with databases, knowledge graphs, vector search (RAG).
    \item \textbf{Adaptability:} One architecture across multiple modalities.
\end{itemize}
\end{frame}

%------------------------------------------------

\begin{frame}{Difficulties: Knowledge}
\begin{itemize}
    \item Need deeper understanding of transformer mechanisms (attention, embeddings).
    \item Evaluation metrics: perplexity, BLEU, ROUGE, accuracy.
    \item Awareness of limitations: hallucinations, bias, lack of transparency.
\end{itemize}
\end{frame}

%------------------------------------------------

\begin{frame}{Difficulties: Skills}
\begin{itemize}
    \item \textbf{MLOps:} Deployment, monitoring drift, latency management.
    \item \textbf{Prompt Engineering and RAG:} Combining symbolic knowledge with model reasoning.
    \item \textbf{Fine-Tuning:} LoRA, PEFT, quantization for efficient adaptation.
    \item \textbf{Data Handling:} Cleaning and curating domain-specific corpora.
\end{itemize}
\end{frame}

%------------------------------------------------

\begin{frame}{Difficulties: Infrastructure}
\begin{itemize}
    \item \textbf{Compute:} Training is costly; inference still GPU-intensive.
    \item \textbf{Pipelines:} Need to process both structured and unstructured data.
    \item \textbf{Security \& Compliance:} Protecting sensitive data.
    \item \textbf{Integration:} Aligning with legacy systems and APIs.
\end{itemize}
\end{frame}

%------------------------------------------------

\begin{frame}{Framing for Engineers}
\begin{itemize}
    \item Transformers are \emph{not magic}, but a new abstraction for processing information.
    \item Apply them at company bottlenecks: support tickets, document processing, anomaly detection.
    \item Start small: prototypes $\rightarrow$ production integration.
    \item Success requires collaboration: engineers, domain experts, ML specialists.
\end{itemize}
\end{frame}

%------------------------------------------------

\begin{frame}{Closing Message}
\begin{itemize}
    \item Transformers are becoming \textbf{general-purpose engines} for pattern recognition.
    \item They provide leverage where rules and heuristics fail.
    \item Adoption is not just about technology but about building:
    \begin{itemize}
        \item Knowledge
        \item Skills
        \item Infrastructure
    \end{itemize}
    \item Goal: make them reliable at enterprise scale.
\end{itemize}
\end{frame}


\begin{frame}{Pipeline Goals}
\begin{itemize}
    \item \textbf{Relevance:} return truly useful chunks for LLM retrieval.
    \item \textbf{Freshness:} keep indexes up-to-date with changing data.
    \item \textbf{Traceability:} every chunk linked to source + version.
    \item \textbf{Scalability \& Latency:} handle growth and meet SLOs.
    \item \textbf{Security:} compliance, audit, access control.
    \item \textbf{Cost-effectiveness:} balance compute vs storage.
\end{itemize}
\end{frame}

%------------------------------------------------

\begin{frame}{Data Sources}
\begin{itemize}
    \item Documents: PDFs, HTML, Word, Markdown.
    \item Databases: product DBs, transactional systems (CDC).
    \item Logs \& telemetry: server logs, sensors, time-series.
    \item Code repositories: Git, issues, PRs.
    \item Communication: emails, tickets, chat transcripts.
    \item APIs: third-party SaaS, exports.
\end{itemize}
\end{frame}

%------------------------------------------------

\begin{frame}{End-to-End Stages}
\begin{enumerate}
    \item Ingestion (batch, streaming, CDC).
    \item Extraction \& normalization (text cleaning, schema mapping).
    \item Deduplication \& filtering (hashing, PII removal).
    \item Chunking (200--1000 tokens, overlap 50--200).
    \item Embedding generation (batching, caching, versioning).
    \item Indexing in vector DB (HNSW, hybrid search).
    \item Retrieval + optional reranking.
    \item Prompt construction + LLM orchestration.
    \item Post-processing: source attribution, feedback loop.
\end{enumerate}
\end{frame}

%------------------------------------------------

\begin{frame}{Chunking \& Embeddings}
\begin{itemize}
    \item \textbf{Chunking:} split into semantically coherent units.
    \begin{itemize}
        \item Typical size: 512 tokens with overlap.
        \item Special rules for code, tables, structured docs.
    \end{itemize}
    \item \textbf{Embeddings:}
    \begin{itemize}
        \item Batched inference on GPU.
        \item Idempotent: embed only once per model+chunk.
        \item Versioning: reindex on model upgrades.
    \end{itemize}
\end{itemize}
\end{frame}

%------------------------------------------------

\begin{frame}{Indexing \& Retrieval}
\begin{itemize}
    \item Store vectors + metadata (doc\_id, offsets, model version).
    \item Vector DB: Qdrant, Milvus, Pinecone, Weaviate, Elasticsearch.
    \item Hybrid retrieval: combine BM25 + vector similarity.
    \item Rerankers: cross-encoders for higher precision.
    \item Filters: metadata-based restrictions (region, product, sensitivity).
\end{itemize}
\end{frame}

%------------------------------------------------

\begin{frame}{Prompt Orchestration}
\begin{itemize}
    \item Assemble top-K chunks into prompt context.
    \item Manage token budget: system prompt + context + answer.
    \item Add explicit citations (provenance).
    \item Fallbacks for low-confidence retrieval.
    \item Human-in-the-loop for sensitive outputs.
\end{itemize}
\end{frame}

%------------------------------------------------

\begin{frame}{Operational Concerns}
\begin{itemize}
    \item \textbf{Compute:} GPU for embeddings, index scaling.
    \item \textbf{Storage:} cold originals (S3/Blob), hot vectors.
    \item \textbf{Governance:} PII redaction, access control, audit logs.
    \item \textbf{Observability:} monitor ingestion lag, retrieval precision@k, LLM latency.
    \item \textbf{Testing:} regression suites + human evaluation.
\end{itemize}
\end{frame}

%------------------------------------------------

\begin{frame}{Best Practices}
\begin{itemize}
    \item Start small, iterate: prototype on 1--2 sources.
    \item Keep originals immutable, always version embeddings.
    \item Use hybrid retrieval for robustness.
    \item Constrain LLMs: encourage extractive answers.
    \item Collect feedback to improve rerankers and retrievers.
    \item Control costs: batch embeddings, quantize vectors.
\end{itemize}
\end{frame}

%------------------------------------------------

\begin{frame}{Common Pitfalls}
\begin{itemize}
    \item \textbf{Under/over-chunking} $\rightarrow$ poor retrieval.
    \item \textbf{No metadata filtering} $\rightarrow$ irrelevant results.
    \item \textbf{Silent re-embedding} $\rightarrow$ inconsistency.
    \item \textbf{Blind trust in LLMs} $\rightarrow$ hallucinations.
    \item \textbf{Stale indices} $\rightarrow$ outdated answers.
\end{itemize}
\end{frame}

%------------------------------------------------

\begin{frame}{Quick Checklist}
\begin{enumerate}
    \item Select 1--2 data sources (e.g., tickets + docs).
    \item Ingest, clean, and chunk into 512 tokens.
    \item Generate embeddings and store with metadata.
    \item Index in a vector DB; expose retriever API.
    \item Connect retriever $\rightarrow$ prompt $\rightarrow$ LLM.
    \item Instrument precision@k, latency, token cost.
    \item Iterate with rerankers and user feedback.
\end{enumerate}
\end{frame}

%------------------------------------------------

\begin{frame}{Closing Message}
\begin{itemize}
    \item Data pipelines are the backbone of RAG.
    \item Success requires \textbf{knowledge, skills, and infrastructure}.
    \item Transformers are powerful, but only as good as the data pipeline behind them.
    \item Goal: reliable, scalable, compliant enterprise knowledge systems.
\end{itemize}
\end{frame}

%------------------------------------------------
\end{document}

