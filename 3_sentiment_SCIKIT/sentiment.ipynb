{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
      "                ('clf', LogisticRegression(max_iter=200))])\n"
     ]
    }
   ],
   "source": [
    "# 1) Example data (replace with your own)\n",
    "X = [\n",
    "    \"Loved it! What a fantastic movie.\",\n",
    "    \"Absolutely terrible. Waste of time.\",\n",
    "    \"Great acting and plot. Highly recommended!\",\n",
    "    \"I hated every minute of it.\",\n",
    "    \"It was okay, not great but not bad either.\",\n",
    "    \"Brilliant direction and beautiful soundtrack.\",\n",
    "    \"Awful experience, I left the theater early.\",\n",
    "    \"Mediocre at best.\",\n",
    "    \"One of the best films this year!\",\n",
    "    \"Not for me.\"\n",
    "]\n",
    "\n",
    " # Binary labels: 1 = positive, 0 = negative\n",
    "y = np.array([1,0,1,0,1,1,0,0,1,0])\n",
    "\n",
    "# Train/validation split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Build a Pipeline: TF-IDF (transformer) -> Logistic Regression (classifier)\n",
    "pipe = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer()),\n",
    "    (\"clf\", LogisticRegression(max_iter=200, n_jobs=None))  # n_jobs not used by liblinear, set solver below\n",
    "])\n",
    "print(pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) Handle class imbalance (if any)\n",
    "classes = np.unique(y_train)\n",
    "class_weights = compute_class_weight(class_weight=\"balanced\", classes=classes, y=y_train)\n",
    "cw_dict = {c: w for c, w in zip(classes, class_weights)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'clf__C': 0.25, 'clf__class_weight': {0: 1.1666666666666667, 1: 0.875}, 'clf__solver': 'liblinear', 'tfidf__max_df': 0.9, 'tfidf__max_features': None, 'tfidf__min_df': 1, 'tfidf__ngram_range': (1, 1)}\n",
      "Best CV accuracy: 0.4167\n"
     ]
    }
   ],
   "source": [
    "# 4) Hyperparameter search (small grid to keep it fast)\n",
    "param_grid = {\n",
    "    \"tfidf__ngram_range\": [(1,1), (1,2)],\n",
    "    \"tfidf__min_df\": [1, 2],\n",
    "    \"tfidf__max_df\": [0.9, 1.0],\n",
    "    \"tfidf__max_features\": [None, 5000],\n",
    "    \"clf__solver\": [\"liblinear\", \"lbfgs\"],  # liblinear good for small/medium; lbfgs supports L2\n",
    "    \"clf__C\": [0.25, 1.0, 4.0],\n",
    "    \"clf__class_weight\": [cw_dict, None],\n",
    "}\n",
    "search = GridSearchCV(\n",
    "    pipe,\n",
    "    param_grid,\n",
    "    scoring=\"accuracy\",\n",
    "    cv=2,\n",
    "    n_jobs=-1,\n",
    "    verbose=0\n",
    ")\n",
    "search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best params:\", search.best_params_)\n",
    "print(\"Best CV accuracy:\", round(search.best_score_, 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test accuracy: 0.3333\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5000    0.5000    0.5000         2\n",
      "           1     0.0000    0.0000    0.0000         1\n",
      "\n",
      "    accuracy                         0.3333         3\n",
      "   macro avg     0.2500    0.2500    0.2500         3\n",
      "weighted avg     0.3333    0.3333    0.3333         3\n",
      "\n",
      "Confusion matrix:\n",
      " [[1 1]\n",
      " [1 0]]\n"
     ]
    }
   ],
   "source": [
    "# 5) Evaluate on test set\n",
    "best_model = search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "print(\"\\nTest accuracy:\", round(accuracy_score(y_test, y_pred), 4))\n",
    "print(\"\\nClassification report:\\n\", classification_report(y_test, y_pred, digits=4))\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text: What a masterpiece! I cried twice.\n",
      "Predicted: positive | Probabilities: [0.48710935 0.51289065]\n",
      "\n",
      "Text: This was unwatchable and boring.\n",
      "Predicted: positive | Probabilities: [0.48045467 0.51954533]\n"
     ]
    }
   ],
   "source": [
    "# 6) Use the model\n",
    "samples = [\n",
    "    \"What a masterpiece! I cried twice.\",\n",
    "    \"This was unwatchable and boring.\"\n",
    "]\n",
    "pred = best_model.predict(samples)\n",
    "proba = best_model.predict_proba(samples)  # probability for each class\n",
    "for text, p, pr in zip(samples, pred, proba):\n",
    "    print(f\"\\nText: {text}\\nPredicted: {'positive' if p==1 else 'negative'} | Probabilities: {pr}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved model to sentiment_lr_tfidf.joblib\n"
     ]
    }
   ],
   "source": [
    "# 7) Persist model to disk\n",
    "joblib.dump(best_model, \"sentiment_lr_tfidf.joblib\")\n",
    "print(\"\\nSaved model to sentiment_lr_tfidf.joblib\")\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model predicts: [0]\n"
     ]
    }
   ],
   "source": [
    "# 8) Load later and predict\n",
    "loaded = joblib.load(\"sentiment_lr_tfidf.joblib\")\n",
    "print(\"Loaded model predicts:\", loaded.predict([\" awful\"]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
